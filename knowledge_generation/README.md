
# Persona Knowledge Generalization

This folder contains the source code of our experiments in **PeaCoK Section 6: Generalizing Persona Knowledge**.
The `baseline` directory contains code modified from the [alexa-with-dstc9-track1-dataset repository](https://github.com/alexa/alexa-with-dstc9-track1-dataset.git).

## Development Environment

- OS: Ubuntu 20.04 LTS (64bit)
- GPU: Nvidia Titan X
- CUDA 11.6
- Python 3.8

## Requirements

You need two environments. (1) Python 3.10.8 for DeBERTa training, Comet-BART & GPT3 tail generation, and DeBERTa ranking (2) Python 3.6 for NLG evaluation.

Pip install all of the requirements, feel free to use another package manager:

```shell
conda create -n peakenv310 python=3.10.8
conda activate peakenv310
pip install -r requirements_python3-10.txt
```

```shell
conda create -n peakenv36 python=3.6
conda activate peakenv36
pip install -r requirements_python3-6.txt
pip install git+https://github.com/Maluuba/nlg-eval.git@master
nlg-eval --setup
```

## Data and Model Download:

### Datasets

Our preprocessed PeaCoK data for persona knowledge generalization experiments can be downloaded from [this link1](https://drive.google.com/file/d/1iVODYzLWtWZNV_wDXxDiRSHZglcjGnV3/view?usp=sharing) and the ready-generated attributes [from this link2](https://drive.google.com/file/d/15ybkMlet2ZsjRUxMJyxECSSr2cg_Pdff/view?usp=sharing), please place `data_persona_gen.zip` and `generated_tails.zip` under this directory and unzip the files. You can also download through the commandline with the following commands: 

```shell
wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=19oapPUb5T-GqcZ0oOpzTTrnzDWyLlB3f' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=19oapPUb5T-GqcZ0oOpzTTrnzDWyLlB3f" -O data_persona_gen.zip && rm -rf /tmp/cookies.txt
unzip data_persona_gen.zip
rm -rf data_persona_gen.zip
```


```shell
wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1KdRWZ5YYeev78nRMxMNaEg3QlHDxK5XA' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1KdRWZ5YYeev78nRMxMNaEg3QlHDxK5XA" -O generated_tails.zip && rm -rf /tmp/cookies.txt
unzip generated_tails.zip
rm -rf generated_tails.zip
mv generated_tails/* data_persona_gen/
rm -rf generated_tails/
```

### Model Checkpoints

If you would like to run the model inference part without training models, you need to download our checkpoints for Comet-BART and DeBERTa from [this link](https://drive.google.com/file/d/1AWP9fHlD0aNzxhvGFWp_9wvrnBusJRRc/view?usp=sharing). Please place `model_checkpoints.zip` under this directory and unzip the file. You can also download through the commandline with: 

```shell
wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1NaK2jA9LciEA4mtyAmhTBTXrzBRc3Y5p' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1NaK2jA9LciEA4mtyAmhTBTXrzBRc3Y5p" -O model_checkpoints.zip && rm -rf /tmp/cookies.txt
unzip model_checkpoints.zip
rm -rf model_checkpoints.zip
```

## Model Training
If you would like to train your own models please complete the following steps and make sure you have the python 3.10.8 environment activated. Otherwise, feel free to use the checkpoints you downloaded above.
For evaluating persona knowledge generalization abilities, we first train two models: 
- a BART-based Comet (Comet-BART) knowledge generator
- a DeBERTa discriminator to re-rank the tails generated by Comet-BART and baseline models (i.e., 5-shot GPT-3 and zero-shot GPT-3.5)

Training Comet-BART with [Kogito](https://github.com/epfl-nlp/kogito) toolkit (with suggested default hyperparameters):

```shell
python cometbart_training_kogito.py
```

Generate negative data samples for DeBERTa discriminator training (as well as unique head and relations which will be used for the model inference part):

```shell
python peacok_data_processing.py
```

Entrypoint for training DeBERTa discriminator (you will find the main training script in `baseline/main.py`):

```shell
bash deberta_training.sh
```

## Model Inference

If you want to use our ready-generated attributes, you can skip to the Automatic Evaluation section. If you would like to generate the tails yourself (with Comet-BART or GPT-3), follow the next steps. *Please note that the OpenAI generations may differ in separate runs.* Make sure that you have the python 3.10.8 environment activated.

Make sure you ran the data processing script, if you didn't do it in the Model Training section.
```shell
python peacok_data_processing.py
```

To generate tails with Comet-BART trained on PeaCoK, run the following script:
```shell
python tail_generation_cometbart.py
```

To generate tails with GPT-3, run the following script:
```shell
# For GPT-3
python tail_generation_gpt3.py --openai_api_key=<YOUR_OPENAI_API_KEY> \
    --gpt3_dict_path="data_persona_gen/gpt3_output_dicts.jsonl" \
    --gpt3_text_path="data_persona_gen/gpt3_kg_data.json" \
    --gpt3_top1_text_path="data_persona_gen/gpt3_top1picks.json" \
    --k_shot=5 \
    --gpt3_model_name=davinci

# For GPT-3.5
python tail_generation_gpt3.py --openai_api_key=<YOUR_OPENAI_API_KEY> \
    --gpt3_dict_path="data_persona_gen/gpt35_output_dicts.jsonl" \
    --gpt3_text_path="data_persona_gen/gpt35_kg_data.json" \
    --gpt3_top1_text_path="data_persona_gen/gpt35_top1picks.json" \
    --k_shot=0 \
    --gpt3_model_name=text-davinci-003
```

To rank and choose the best generations, use DeBERTa:
```shell
# For Comet-BART trained on PeaCoK
python deberta_eval.py \
    --generated_instances_path="data_persona_gen/cometbart_kg_data.json" \
    --deberta_top_choice_path="data_persona_gen/cometbart_deberta_top1picks.json"

# For GPT3
python deberta_eval.py \
    --generated_instances_path="data_persona_gen/gpt3_kg_data.json" \
    --deberta_top_choice_path="data_persona_gen/gpt3_deberta_top1picks.json"

# For GPT-3.5
python deberta_eval.py \
    --generated_instances_path="data_persona_gen/gpt35_kg_data.json" \
    --deberta_top_choice_path="data_persona_gen/gpt35_deberta_top1picks.json"
```

## Automatic Evaluation

For the automatic evaluation, make sure you have the python 3.6 environment activated and run the following script:

```shell
# For Comet-BART trained on PeaCoK
python nlg_eval.py \
    --hypothesis_path="data_persona_gen/cometbart_deberta_top1picks.json" \
    --references_path="data_persona_gen/neural_kg_data_test.json" \
    --results_path="data_persona_gen/cometbart_nlg_results.json"

# For GPT3
python nlg_eval.py \
    --hypothesis_path="data_persona_gen/gpt3_deberta_top1picks.json" \
    --references_path="data_persona_gen/neural_kg_data_test.json" \
    --results_path="data_persona_gen/gpt3_nlg_results.json"

# For GPT3.5
python nlg_eval.py \
    --hypothesis_path="data_persona_gen/gpt35_deberta_top1picks.json" \
    --references_path="data_persona_gen/neural_kg_data_test.json" \
    --results_path="data_persona_gen/gpt35_nlg_results.json"
```

If you would like to have the SkipThoughts results as well please comment the following in nlg_eval.py at line 58:
```python
 nlgeval.no_skipthoughts = True
```
